HASH_SIZE = "1e9"
METRICS = ['wip', 'ip']
SETS = {}
ALLRUNS = set()
RUNPROJECTS = {}
for project, setruns in config.items():
    for setname, runs in setruns.items():
        SETS[setname] = runs
        for r in runs:
            ALLRUNS.add(r)
            RUNPROJECTS[r] = project

onsuccess:
    shell("mail -s 'Workflow complete' kevin.murray@anu.edu.au < {log}")
onerror:
    shell("mail -s 'Workflow error' kevin.murray@anu.edu.au < {log}")


## BEGIN RULES
rule all:
    input:
        ["data/sra/{project}/{run}.sra".format(run=run, project=project)
         for run, project in RUNPROJECTS.items()],
        expand("data/kwip/{set}_{metric}.{mat}", set=SETS.keys(), metric=METRICS, mat=["dist", "kern"]),

rule clean:
    shell:
        "rm -rf data .snakemake"

rule sra:
    output:
        "data/sra/{project}/{run}.sra",
    log:
        "data/log/getrun/{project}-{run}.log"
    shell:
        "wget -c -O {output}"
        "   https://sra-download.ncbi.nlm.nih.gov/srapub/{wildcards.run}"
        " >{log} 2>&1"

rule sicklereads:
    input:
        "data/sra/{project}/{run}.sra",
    output:
        temp("data/tmp/reads/{project}/{run}_il.fastq"),
    log:
        "data/log/sickle/{project}-{run}.log"
    shell:
        "( fastq-dump"
        "   --split-spot"
        "   --skip-technical"
        "   --stdout"
        "   --readids"
        "   --defline-seq '@$sn/$ri'"
        "   --defline-qual '+'"
        "   {input}"
        "| sickle se"
        "   -t sanger"
        "   -q 28"
        "   -l 1"  # Keep all reads
        "   -f /dev/stdin"
        "   -o {output}"
        ") >{log} 2>&1"


rule hash:
    input:
        "data/tmp/reads/{project}/{run}_il.fastq"
    output:
        "data/counts/{project}/{run}.ct.gz"
    params:
        x=HASH_SIZE,
        N='1',
        k='20',
    threads:
        16
    log:
        "data/log/counts/{project}-{run}.log"
    priority:
        10
    shell:
        "load-into-counting.py"
        "   -N {params.N}"
        "   -x {params.x}"
        "   -k {params.k}"
        "   -T {threads}"
        "   -b"
        "   -s tsv"
        "   {output}"
        "   {input}"
        "   >{log} 2>&1"


rule kwip:
    input:
        lambda wc: ["data/counts/{project}/{run}.ct.gz".format(project=RUNPROJECTS[r], run=r)
                    for r in SETS[wc.set]]
    output:
        d="data/kwip/{set}_{metric}.dist",
        k="data/kwip/{set}_{metric}.kern"
    params:
        metric=lambda w: '-U' if w.metric == 'ip' else '',
    log:
        "data/log/kwip/{set}-{metric}.log"
    benchmark:
        "data/benchmarks/kwip/{set}-{metric}.tsv"
    threads:
        16
    shell:
        "kwip"
        " {params.metric}"
        " -d {output.d}"
        " -k {output.k}"
        " -t {threads}"
        " {input}"
        " >{log} 2>&1"


rule kwip_stats:
    input:
        lambda wc: ["data/counts/{project}/{run}.ct.gz".format(project=RUNPROJECTS[r], run=r)
                    for r in SETS[wc.set]]
    output:
        "data/kwip/{set}.stat"
    log:
        "data/log/kwip-stats/{set}.log"
    threads:
        16
    shell:
        "kwip-stats"
        " -o {output}"
        " -t {threads}"
        " {input}"
        " >{log} 2>&1"
